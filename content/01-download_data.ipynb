{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Working with Galv data\n",
    "\n",
    "This notebook demonstrates how to work with Galv data in Python.\n",
    "It closely follows the code example found on the Files page of the Galv app."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Before we can get our hands on data, we need to know where the API is (`host`), who to tell the API we are (`token`), and what data we want (`dataset_ids`).\n",
    "If you're using the public Galv instance at https://galv-backend-dev.fly.dev, you can leave `host` as it is.\n",
    "\n",
    "You can get a token by going to the Galv app, clicking on your name in the top right, and selecting \"API token\" from the dropdown.\n",
    "Then select 'create a new token' and copy the token that is generated.\n",
    "Remember to give your token a sensible expiry date, so you don't stop being authorised to access the data before you're done with it."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "host = \"https://galv-backend-dev.fly.dev\"\n",
    "token = input(\"Provide your API token\")\n",
    "dataset_ids = [input(\"Provide the dataset ID\")]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download data\n",
    "\n",
    "First we need some boilerplate code to cover configuration and importing the necessary libraries."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import duckdb\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"accept\": \"application/json\"\n",
    "}\n",
    "verbose = True\n",
    "dataset_metadata = {}\n",
    "parquets = {}\n",
    "\n",
    "\n",
    "def vprintln(message):\n",
    "    if verbose:\n",
    "        print(message)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Then we can define our `get_dataset` function. \n",
    "This function will download the metadata for a dataset and then download the parquet partitions for that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(id):\n",
    "    vprintln(f\"Downloading dataset {id}\")\n",
    "\n",
    "    response = requests.get(f\"{host}/files/{id}/\", headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching dataset {id}: {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        body = response.json()\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error parsing JSON for dataset {id}\")\n",
    "        return\n",
    "\n",
    "    dataset_metadata[id] = body\n",
    "    parquet_partitions = dataset_metadata[id][\"parquet_partitions\"]\n",
    "    len_partitions = len(parquet_partitions)\n",
    "    vprintln(f\"Downloading {len_partitions} parquet partitions for dataset {id}\")\n",
    "\n",
    "    dataset_dir = tempfile.mkdtemp(prefix=f\"py_{id}\")\n",
    "\n",
    "    for i, pp in enumerate(parquet_partitions):\n",
    "        vprintln(f\"Downloading partition {i + 1} from {pp}\")\n",
    "        partition_response = requests.get(pp, headers=headers)\n",
    "        if partition_response.status_code != 200:\n",
    "            print(f\"Error fetching parquet partition {i + 1} for dataset {id}: {partition_response.status_code}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            parquet_info = partition_response.json()\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error parsing JSON for dataset {id} parquet partition {i + 1}\")\n",
    "            continue\n",
    "\n",
    "        pq_file = parquet_info[\"parquet_file\"]\n",
    "        vprintln(f\"Downloading .parquet from {pq_file}\")\n",
    "        path = os.path.join(dataset_dir, f\"{i + 1}.parquet\")\n",
    "\n",
    "        download_response = requests.get(pq_file, headers=headers)\n",
    "        if download_response.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                f.write(download_response.content)\n",
    "            vprintln(f\"Partition {i + 1} downloaded successfully\")\n",
    "        else:\n",
    "            print(f\"Error downloading .parquet file from {pq_file}: {download_response.status_code}\")\n",
    "\n",
    "    # Add parquet from directory\n",
    "    parquets[id] = duckdb.read_parquet(f\"{dataset_dir}/*.parquet\")\n",
    "    vprintln(\"Completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "And finally we can actually use our download function to get the data."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for id in dataset_ids:\n",
    "    get_dataset(id)\n",
    "    vprintln(f\"Completed dataset {id}\")\n",
    "\n",
    "vprintln(\"All datasets complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the data\n",
    "\n",
    "Most data-oriented tasks are covered in other notebooks, so here we just take a quick look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load a dataset as a DataFrame\n",
    "df = parquets[dataset_ids[0]].read().to_pandas()\n",
    "df"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from IPython.display import JSON\n",
    "JSON(dataset_metadata[dataset_ids[0]], metadata={}, expanded=True, root='test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pyodide)",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
